<!--title={Big-O Notationn}-->

Now that we have an understanding of *time complexity* and *space complexity* and how to express them as functions of time, we can elaborate on the way we express these functions. **Big-O Notation** is a common way to express these functions. 

Instead of using terms like *Linear Time*, *Quadratic Time*, *Logarithmic Time*, etc., we can write these terms in **Big-O Notation**. Depending on what time the function increases by, we assign it a **Big-O** value. **Big-O** notation is incredibly important because it allows us to take a more mathematical and calculated approach to understanding the way functions and algorithms grow. 

**Big-O notation** is also useful because it simplifies how we describe a function's time complexity and space complexity. So far, we have defined a function of time as a number of terms. With Big-O notation, we are able to use only the dominant, or fastest growing, term!

## How to Write in Big-O Notation

To write in **Big-O Notation** we use a capital O followed with parentheses : **O()**. 

Depending on what is inside of the parentheses will tell us what time the function grows in. Let's look at an example. 

```python
value = [1, 2, 3, 4, 5]

def valueSum(value): 
    sum = 0
    for i in value: 
        sum = sum + i
    return sum
```

Going back to our `valueSum` function, we had previously expressed the runtime of this function as input increase.  We had written the expression *Time(Input)* and the components its made of. 

**Time(Input) = c1 + c2*n + c3**

`c1` comes from the line `sum = 0` . We know `sum = 0` will always take the same amount of time to run because we are assigning a value to a variable. We call this *constant time* because no matter what function this line is in, it will take the same amount of time to run. 

Since **c1** runs in constant time, we would write it as **O(1)** in **Big-O Notation**. Notice that the line repeats only once. 

`sum = sum + i` is responsible for **c2** in our equation. We multiply **c2** by *n* however because **c2** is repeated multiple times. The runtime of this function increase linearly depending on the amount of elements that are added to `sum`. 

In **Big-O Notation**, this line would be rewritten as **O(n)** and would inform us that this line runs in *linear time*. We use the dominant term which is **c2 * n**. Since **c2 * n** and **n** behave the same (linearly), we can drop the coefficient of *c2*.

Lastly, `return sum` is written as **O(1)** because it operates under constant time. 

Rewriting our `valueSum` function but in **Big-O notation**, we would have:

```
Time(Input) = O(1) + O(n) + O(1). 
```

We've written the lines of the function in **Big-O Notation**, but now we need to write the function itself in **Big-O Notation**. The way we do that is to choose the term that is growing the fastest in the function (the dominant term) as *n* gets very large. We then disregard any coefficients of that term. Then assign that term for the function. 

For `valueSum`, the fastest growing term is **c2 * n** . If we ignore **c2** , we are left with just `n`. We now know that the time complexity of `valueSum` is **O(n)** and the runtime of `valueSum` grows in a linear fashion.

Let's look at another example.

In the function `listInList`:

```python
keypad = [[1, 2, 3], 
          [4, 5, 6],
          [7, 8, 9],
          [0]] 
def listInList(keypad):
    sum = 0
    for row in keypad:
        for i in row:
            sum += i
    return sum
```

`listInList` expressed as a function of time looks like this:

$$
Time(Input) = c1 + c2 * n^2 + c3
$$
We know that **c1** corresponds to **O(1)** . 
**c2 * n^2** is written as **O(n^2)**. 
**c3** is written as **O(1)** since it operates under constant time.

The fastest growing term is **c2 * n^2**  so we know that this term defines how our function is written in **Big-O Notation**. Dropping the coefficient, **c2**, our function is expressed as **O(n^2)**. in **Big-O Notation**. This tells us that `listInList` grows in quadratic time. 

As for our last example, let's make a quick edit to our function `listInList` and call it `listInList2`. 

```python
keypad = [[1, 2, 3], 
          [4, 5, 6],
          [7, 8, 9],
          [0]]
def listInList2(keypad):
    sum = 0
    for row in keypad:
        for i in row:
            sum += i
    for row in keypad:
        for i in row:
            sum += i
    while sum <= 100:
        sum += 1
    return sum
```

Notice that we doubled the amount of *for loops* in our function. We have to account for that when writing `listInList2` as a function of time. Since we have extra lines in our code, we need to add a term to our **Time(Input)** expression. To account for a double for loop, we can simply add another **n^2 * c2** to our **Time(Input) expression**. 

$$
T(I) = c1 + (c2 * n^2) + (c2 * n^2) + (c3 * n) + c4
$$


The fastest growing term is **n^2 * c2** but we have two of them. Since **n^2 * c2** corresponds to **O(n^2)**, we are left with:

**Time(Input) = O(n^2) + O(n^2)**

This can simply be rewritten as:

**Time(Input) = 2 * O(n^2)**

It might be intuitive to think that our function written in **Big-O notation** would be **2 * O(n^2)** or even **O(2n^2)**, but we have to remember that **Big-O notation only tells us what time a function grows in**. **O(n^2)** and  **O(2n^2)** both grow in the same fashion, quadratic, so we can simply write the function as **O(n^2)**. 

It is very important to remember that Big-O notation is determined by the fastest growing term as `n` gets very large. When n is small, c3 * n will grow faster than c2 * n^2. **Big-O notation is determined when n is very large** because time and space complexities focus on when inputs are very large; not small. 



### Time expressed in Big-O Notation

| Time             | Big O Notation |
| ---------------- | -------------- |
| Constant Time    | O(1)           |
| Linear Time      | O(n)           |
| Quadratic Time   | O(n^2)         |
| Logarithmic Time | O(log n)       |
| Factorial Time   | O(n!)          |

In the table, there are some other times you should know of. The graph lists the growth of functions from slowest to fastest. A function in **O(1)**'s runtime grows the slowest, and a function in **O(n!)** grows the fastest. Needless to say, we want our functions to be **constant or linear time.** 

